---
title: "Cereal Data Factor Analysis"
output: word_document
---

```
{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Loading the data set into R

```{r}
#library(readr)
cereal <- read.csv("Dataset_Cereal.csv")
View(cereal)
head(cereal,10)
attach(cereal)
```

## Doing Exploratory Data Analysis

Summary of the data

```{r}
summary(cereal)
```


##Structure of the data
```{r}
str(cereal)
```

It has been observed that there are 235 observations and 26 variables of the 12 different brands of cereals. All the attributes are 'ordinal' and has been marked on the scale between 1-5, but few variables mistakenly entered as 6. Hence, need to be corrected.

```{r}
cereal[cereal==6] <- 5
```

Create a list of cross tables of each independent columns with target variable.

```{r}
list.of.xtabs <- lapply(cereal[,c(colnames(cereal))[2:26]], function(x) xtabs(~ Cereals + x))

list.of.xtabs[0:3]
```

##chi square test of independence to find the relation btw independent and target variable

```{r}
ix=0
for(cols in list.of.xtabs[1:3]){
 ix=ix+1
 if(chisq.test(cols)$p.value<0.05){
   print(names(list.of.xtabs[ix]))
   print(paste("P values with 95% significance level:",chisq.test(cols)$p.value)) 
 }
}
```
Hence it can be seen from the above result that independent variables are correlated with target variable with 95% confidence interval

Now we will check for correlation between independent variables. We will check for Filling below

```{r}

chisq.test(xtabs(~Cereals+Filling))
chisq.test(xtabs(~Cereals+Natural))
chisq.test(xtabs(~Cereals+Fibre))
chisq.test(xtabs(~Cereals+Sugar))
```

From the above figure, it is clear that the most of the variables are fairly correlated.Therefore, factor analysis has to be done.

##Testing the assumptions to do factor analysis

###Kaiser-Meyer-Olkin (KMO) 

KMO Test is a measure of how suited your data is for Factor Analysis. The test measures sampling adequacy for each variable in the model and for the complete model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the more suited your data is to Factor Analysis.

KMO returns values between 0 and 1. A rule of thumb for interpreting the statistic:

  1. KMO values between 0.8 and 1 indicate the sampling is adequate.
  2. KMO values less than 0.6 indicate the sampling is not adequate and that remedial action should be taken. Some authors put this value at 0.5, so use your own judgment for values between 0.5 and 0.6.
  3. KMO Values close to zero means that there are large partial correlations compared to the sum of correlations. In other words, there are widespread correlations which are a large problem for factor analysis.


```{r}
library(psych)
cereal1 <- cereal[,-1]
cereal.KMO <- KMO(cor(cereal1))
cereal.KMO
```

Ideally, the overall MSA value has to be greater than 0.5 from KMO test to do factor analysis.

For reference, Kaiser put the following values on the results:

  0.00 to 0.49 unacceptable.
  0.50 to 0.59 miserable.
  0.60 to 0.69 mediocre.
  0.70 to 0.79 middling.
  0.80 to 0.89 meritorious.
  0.90 to 1.00 marvelous.

##Bartlett's Test of Sphericity

```{r}
cereal.barlett <- cortest.bartlett(cereal1, nrow(cereal))
cereal.barlett
```

Bartlett's test of sphericity tests the hypothesis that the correlation matrix is an identity matrix, which would indicate that the variables are unrelated and therefore unsuitable for structure detection. 
Small values (less than 0.05) of the significance level indicate that a factor analysis may be useful with your data. 

Thus, based on the above results, we assume that the fair correlation exists between all the variables, so we can run the Factor Analysis on the given data set.


##Running principal Component Analysis to identify the number of factors

```{r}
cereal.pca <- princomp(cereal1,scores = TRUE, cor = TRUE)
summary(cereal.pca)
```

###Interpretations from the above values - 

According to KMO concept, all principal component values which has an Eigen value of greater than 1, can be taken into consideration for the dimension reduction technique.

Based on this criteria, in our situation we will take first 5 principal components into consideration.

It is evident that from the output values- 
  1. Comp 1 explains 26% of variation of the whole data set
  2. Comp 2 explains 15%
  3. Comp 3 explains 10%
  4. Comp 4 explians 7% and 
  5. Comp 5 explains 4% of variation of the entire data set.
  
  Thus, PCA1 to PCA5 principal components are able to explain 62% of the data.

##Plot of Principal Components with variance

```{r}
plot(cereal.pca,col = "black")
```

From the above plot, we see that after Comp 5, the bar flattens and there is no further steep observed.

##Screeplot of Principal Components vs Variances

```{r}
screeplot(cereal.pca,type = "lines",main = "Screeplot")
```

In the above Screeplot , we see that after the Comp 5 which is above Eigen value(Variances) 1,the "Elbow" shape becomes a line and flattens.

```{r}
loadings(cereal.pca)

print(cereal.pca$loadings, cutoff = 0.2)
```
##Biplot of components
```{r}
biplot(cereal.pca, choices = c(1,2))
```

From the loadings, we are able to interpret that 
Filling,Natural,Fibre,Satisfying,Energy,Health,Regular,Quality, Nutritious 
are the variables that are added and gives the weightage to Comp 1.

Similarly, we can identify the variables that adds weightage in the Comps 2,3,4,5.


##Running Factor Analysis

Factor analysis without rotation
```{r}
solution <- fa(r=cor(cereal1), nfactors = 5, rotate = "None", fm = "pa")
solution$loadings
```

We also observe that the loadings are not very clear, hence it has to be rotated to increase the interpretability. It is done to maximize the "high loadings" and minimize the "low loadings".

##Factor loadings with Varimax rotation

```{r}
solution1 <- fa(r=cor(cereal1), nfactors = 5, rotate = "varimax", fm = "pa")
print(solution1)
print(solution1$loadings)

fa.diagram(solution1)

```

##Factor loadings with oblimin since factors are correlated

```{r}
#Oblique rotation (oblimin)
solution2 <- fa(r=cor(cereal1), nfactors = 5, rotate = "oblimin", fm = "pa")
print(solution2)
fa.diagram(solution2)
```

Thus, after the rotation we observe that the 5 factors are able to explain about 52% of the data. 
Based on this, we are able to infer the latent relation between the variables they are grouped into as "Factors". 
The factor loadings explains the weightage of each variable has in the Factors. 
We can also see that the some of the variables are present in more than one factor, which is known as the 'Communality' effect.

Thus,the factor scores for all the 235 observations are known, from which prediction or classification can be done.

##Naming and defining the factors

##Factor 1 - Cereals Benefits

Fibre,Health,Nutritious,Energy,Satisfying,Quality,Filling

##Factor 2 - Cereals Ingredients

Sweet, Salt, Calories, Sugar, Process

##Factor 3 - Cereals tastyness

Crisp,Treat

##Factor 4 - Cereals intake people type

Kids, Economical, Family

##Factor 5 - Cereal type negative's

Plain,boring